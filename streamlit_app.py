# streamlit_app.py
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from functools import lru_cache  # still unused but harmless

# ---------- configuration ----------
MODEL_PATH = "nawafalomari/ICS606"      # ğŸ’¾ your HF repo
LABELS     = {0: "Human ğŸ™‹ğŸ»â€â™‚ï¸", 1: "AI ğŸ¤–"}       # ğŸ·ï¸ customise if you add more classes

# ---------- helpers ----------
@st.cache_resource(show_spinner="Loading modelâ€¦")
def load_model():
    tok = AutoTokenizer.from_pretrained(MODEL_PATH)
    mdl = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    mdl.eval()                                      # weâ€™re only doing inference
    return tok, mdl


def predict(code: str) -> dict[str, float]:
    """
    Returns a dictÂ {label: prob} with probabilities in the rangeÂ [0,Â 1].
    """
    tokenizer, model = load_model()

    with torch.inference_mode():
        inputs = tokenizer(
            code,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt",
        )
        logits = model(**inputs).logits             # shapeÂ [1,Â num_labels]
        probs  = torch.softmax(logits, dim=-1)[0]   # shapeÂ [num_labels]

    # map to readable labels and convert to standard Pythonâ€¯floats
    return {LABELS[i]: float(probs[i]) for i in range(len(probs))}

# ---------- UI ----------
st.title("GPTPointerException ğŸ¤–")
code_snippet = st.text_area("Paste a code diff or snippet:", height=200)

if st.button("Classify") and code_snippet.strip():
    probs = predict(code_snippet)

    # 1ï¸âƒ£Â show the argâ€‘max label + confidence
    best_label = max(probs, key=probs.get)
    best_pct   = probs[best_label] * 100
    st.success(f"**{best_label}**â€¯Â·â€¯{best_pct:.2f}% confidence")

    # 2ï¸âƒ£Â (optional) show a full breakdown
    st.subheader("Class probabilities")
    for lbl, p in probs.items():
        st.write(f"- {lbl}: {p*100:.2f}%")
        # or:  st.metric(lbl, f"{p*100:.2f}%")
